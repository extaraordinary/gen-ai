{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Large Language Models (LLMs)\n",
    "\n",
    "In this notebook, we will explore how to use a pre-trained LLM for text generation. Specifically, we will use GPT-2 to generate coherent and contextually relevant text based on an input prompt. Through this notebook, we will:\n",
    "- Load a pre-trained LLM and its tokenizer.\n",
    "- Define a function to generate text using the model.\n",
    "- Experiment with different parameters to influence the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Set Up the Environment\n",
    "\n",
    "First and foremost, one must import the necessary libraries and set up the environment. In this particular case, we will use the `transformers` library from Hugging Face, which provides a simple interface for working with pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "Then, we load the pre-trained GPT-2 model and its corresponding tokenizer. The tokenizer is responsible for converting text into the format that the model can understand, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9485af76c93342219c148a2ed7ce3c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cd613df9da478e9f4cf12ae99fb345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3757a506d0574be996d3672cc35d0f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d801700279f74d7ba08a8e77906b4453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fe14b30de44b328225d6a10487bab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99936b96b0cf40049e9462fa22a7620a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdf8bab49474caea8d8dbb0ec33ccf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained GPT-2 model and its tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Function\n",
    "\n",
    "Here, we define a function that takes a text prompt as input and generates a continuation using the GPT-2 model. The function allows for customization of various parameters such as the maximum length of the generated text and the number of different sequences to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=50, num_return_sequences=1, temperature=1.0, top_p=1.0, top_k=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)    # tokenize and prepare the prompt for the model\n",
    "    \n",
    "    outputs = model.generate(    # generate text using the model\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=2,\n",
    "        repetition_penalty=1.5,\n",
    "        do_sample=True,    # allows sampling and diversity in output\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "        pad_token_id=tokenizer.eos_token_id)    # prevents warnings\n",
    "    \n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]    # decode and return the generated text\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Examples: Generate and Display Text\n",
    "\n",
    "Let's test our text generation function with a few example prompts. We will input a starting sentence, and the model will generate the rest of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1:\n",
      "Once upon a time in a land far away, his own son was found dead at sea. His family were unable to find him alive – until they tried searching from coastlines and forests for clues as to what had happened on the night of 23 March\n",
      "\n",
      "Generated Text 2:\n",
      "Once upon a time in a land far away I heard the voice of Mather, and he took her out into space. All my father knew so soon as his eyes closed was from above that it would take me longer to say what she had said\n",
      "\n",
      "Generated Text 3:\n",
      "Once upon a time in a land far away, they shall be looked on as the savages. Now is it that our fathers were at war when many nations fought amongst themselves? Yet there never was an Englishman who had no such feeling! He\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time in a land far away\"\n",
    "generated_texts = generate_text(prompt, max_length=50, num_return_sequences=3)\n",
    "\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated Text {i+1}:\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation & Exploring Different Parameters\n",
    "\n",
    "Experiment with different prompts and parameters like `max_length`, `temperature`, and `top_k` to see how they affect the generated text. By tweaking these, one can influence the creativity and coherence of the generated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1:\n",
      "In a futuristic world where AI controls everything, this is the perfect choice for our future.\n",
      "Ships are made from materials that have been carefully selected to be suitable and durable enough to withstand some of today's most extreme conditions - all while also being able (and willing) in any situation when necessary to perform their job safely at an affordable price point.\"\n",
      "\n",
      "Experiment 2:\n",
      "In a futuristic world where AI controls everything, there's no reason to worry about anything.\n",
      " The new version of the game is more accessible and fun than ever before thanks to its deep story-telling mechanics that can't be beat by simple exposition or handholding - you'll have access only in multiplayer mode (with an additional $9 fee) but it also supports both PC and PS4 versions on Xbox One as well!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment_prompt = \"In a futuristic world where AI controls everything\"\n",
    "experiment_texts = generate_text(experiment_prompt, max_length=100, num_return_sequences=2, temperature=0.7, top_p=0.9)\n",
    "\n",
    "for i, text in enumerate(experiment_texts):\n",
    "    print(f\"Experiment {i+1}:\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing the Output\n",
    "\n",
    "Improving the coherence and quality of the generated text from a large language model involves fine-tuning various aspects of the model's output settings, and possibly even the model itself. Some strategies are:\n",
    "\n",
    "1. Adjust Sampling Parameters\n",
    "\n",
    "    • **Temperature:** Lowering the temperature value (e.g., between 0.7 to 0.9) can make the text more focused and deterministic, leading to more coherent sentences. However, too low a temperature (e.g., below 0.5) might result in repetitive or overly conservative text.\n",
    "\n",
    "    • **Top-p (Nucleus Sampling):** Top-p sampling (with a value of, e.g., 0.9) considers the smallest set of tokens whose cumulative probability adds up to 0.9. This filters out unlikely options and keeps the text more coherent.\n",
    "\n",
    "    • **Top-k:** Top-k sampling (with a value of, e.g., 50) restricts the number of tokens considered at each step to the top 50, which can help maintain coherence by focusing on the most likely options.\n",
    "    \n",
    "\n",
    "2. Increase the Context Size\n",
    "\n",
    "    • **Longer prompts:** Providing a more detailed prompt or context can guide the model to generate more contextually relevant texts. Instead of starting with a short phrase, you could give a paragraph that sets the scene or introduces key concepts.\n",
    "\n",
    "    • **Sliding window approach:** For generating longer texts, consider feeding the model its previous outputs as part of the prompt in subsequent iterations.\n",
    "\n",
    "\n",
    "3. Fine-Tune the Model on Specific Data\n",
    "\n",
    "    • **Domain-specific fine-tuning:** For specific types of content with high coherence (e.g., technical writing, storytelling), fine-tune the pre-trained model on a dataset that matches desired output style can significantly improve results.\n",
    "\n",
    "    • **Training with Reinforcement Learning:** Techniques like Reinforcement Learning from Human Feedback (RLHF) can be used to adjust the model's behavior based on specific quality metrics.\n",
    "\n",
    "\n",
    "4. Iterative Prompting\n",
    "\n",
    "    • **Iterative prompt refinement:** After generating text, modify the prompt based on the output received and rerun the generation.\n",
    "\n",
    "\n",
    "5. Post-Processing the Generated Text\n",
    "\n",
    "    • **Text editing:** After generating text, you can apply rules or heuristics to clean up or adjust the output. This might include correcting grammar, removing contradictions, or merging sentences for better flow.\n",
    "\n",
    "    • **Chaining outputs:** Generate multiple completions and then manually or programmatically select and combine the best parts. This allows the construction of a more coherent narrative.\n",
    "\n",
    "\n",
    "6. Experiment with Beam Search\n",
    "\n",
    "    • **Beam Search:** Beam search (num_beams=>3) is a technique where the model explores multiple paths before finalizing on the most coherent one. This often leads to better-structured sentences and paragraphs.\n",
    "\n",
    "\n",
    "7. Use Larger or Newer Models\n",
    "\n",
    "    • **Try GPT-3 or GPT-4:** Consider using GPT-3 or GPT-4, which are more advanced and capable of generating contextually appropriate text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps...\n",
    "\n",
    "- Fine-tuning the model on a custom dataset to generate domain-specific text.\n",
    "- Exploring more advanced models like GPT-3 or GPT-4.\n",
    "- Implementing a user interface for interactive text generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
