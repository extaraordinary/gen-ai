{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Large Language Models (LLMs)\n",
    "\n",
    "In this notebook, we will explore how to use a pre-trained LLM for text generation. Specifically, we will use GPT-2 to generate coherent and contextually relevant text based on an input prompt. Through this notebook, we will:\n",
    "- Load a pre-trained LLM and its tokenizer.\n",
    "- Define a function to generate text using the model.\n",
    "- Experiment with different parameters to influence the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Set Up the Environment\n",
    "\n",
    "First and foremost, one must import the necessary libraries and set up the environment. In this particular case, we will use the `transformers` library from Hugging Face, which provides a simple interface for working with pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "Then, we load the pre-trained GPT-2 model and its corresponding tokenizer. The tokenizer is responsible for converting text into the format that the model can understand, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained GPT-2 model and its tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Function\n",
    "\n",
    "Here, we define a function that takes a text prompt as input and generates a continuation using the GPT-2 model. The function allows for customization of various parameters such as the maximum length of the generated text and the number of different sequences to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=50, num_return_sequences=1, temperature=1.0, top_p=1.0, top_k=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)    # tokenize and prepare the prompt for the model\n",
    "    \n",
    "    outputs = model.generate(    # generate text using the model\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=2,\n",
    "        repetition_penalty=1.5,\n",
    "        do_sample=True,    # allows sampling and diversity in output\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "        pad_token_id=tokenizer.eos_token_id)    # prevents warnings\n",
    "    \n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]    # decode and return the generated text\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Examples: Generate and Display Text\n",
    "\n",
    "Let's test our text generation function with a few example prompts. We will input a starting sentence, and the model will generate the rest of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1:\n",
      "Once upon a time in a land far away I became known as something of an alien, but for whatever reason it's so rare these days to see myself or be able even one bit like me because they don't know how and if anyone can possibly\n",
      "\n",
      "Generated Text 2:\n",
      "Once upon a time in a land far away, humans were able to do whatever they needed without them knowing what it was like. At one point the world collapsed into chaos before all of humanity could realize that this planet as we know itself lacked natural resources\n",
      "\n",
      "Generated Text 3:\n",
      "Once upon a time in a land far away from us, we all took part. If he had to die for what others have brought… then I wonder if that is it?\" Merely being told the truth she looked up at Dumbledore's lips\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time in a land far away\"\n",
    "generated_texts = generate_text(prompt, max_length=50, num_return_sequences=3)\n",
    "\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated Text {i+1}:\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation & Exploring Different Parameters\n",
    "\n",
    "Experiment with different prompts and parameters like `max_length`, `temperature`, and `top_k` to see how they affect the generated text. By tweaking these, one can influence the creativity and coherence of the generated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1:\n",
      "In a futuristic world where AI controls everything, there are no rules. Humans have to follow the laws of physics; their bodies cannot move at will and they must be restrained from touching each other (but not touch me).\n",
      "I am one of those who believe that this is possible only with science-based solutions! If you're willing - if your body can control it's going down without any problems because all we need is an understanding or something like \"we've got nothing left\". But what about\n",
      "\n",
      "Experiment 2:\n",
      "In a futuristic world where AI controls everything from cars to food, the human brain is able only in emergencies.\n",
      " (Photo: Marvel Comics) Story Highlights The characters are created by Scott Snyder and Mark Millar with help of Joe Quesada as well Assemble Your Own Brain - \"The Avengers\" series focuses on an unlikely team who have been forced into battle against their own superpowers This week's issue will feature one-shots featuring some familiar faces including Thor, Hulk, XO Manow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment_prompt = \"In a futuristic world where AI controls everything\"\n",
    "experiment_texts = generate_text(experiment_prompt, max_length=100, num_return_sequences=2, temperature=0.7, top_p=0.9)\n",
    "\n",
    "for i, text in enumerate(experiment_texts):\n",
    "    print(f\"Experiment {i+1}:\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=200, temperature=0.9, top_p=0.95, top_k=60, num_return_sequences=3, num_beams=7):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=2,\n",
    "        repetition_penalty=1.5,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "        num_beams=num_beams,  # beam search\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1:\n",
      "The future of technology is filled with both incredible possibilities and unforeseen challenges. Scientists and engineers around the world are working on innovations that could change our lives forever. One such innovation is artificial intelligence, or AI.\n",
      "\n",
      "AI is a new form of machine learning that can be used to solve complex problems. It has the potential to revolutionize the way we think about information and how we interact with it. In this article, we will look at some of the most exciting developments in AI and its potential applications.\n",
      "\n",
      "Output 2:\n",
      "The future of technology is filled with both incredible possibilities and unforeseen challenges. Scientists and engineers around the world are working on innovations that could change our lives forever. One such innovation is artificial intelligence, or AI.\n",
      "\n",
      "AI is a new form of machine learning that can be used to solve complex problems. It has the potential to revolutionize the way we think about information and how we interact with it. In this article, we will look at some of the most exciting developments in AI and its potential applications. We will also explore the ways in which AI can help us better understand our everyday lives.\n",
      "\n",
      "Output 3:\n",
      "The future of technology is filled with both incredible possibilities and unforeseen challenges. Scientists and engineers around the world are working on innovations that could change our lives forever. One such innovation is artificial intelligence, or AI.\n",
      "\n",
      "AI is a new form of machine learning that can be used to solve complex problems. It has the potential to revolutionize the way we think about information and how we interact with it. In this article, we will look at some of the most exciting developments in AI and its potential applications. We will also explore the ways in which AI can help us make better decisions about what we want to do with our data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of technology is filled with both incredible possibilities and unforeseen challenges. Scientists and engineers around the world are working on innovations that could change our lives forever. One such innovation is\"\n",
    "generated_texts = generate_text(prompt, max_length=200, temperature=0.9, top_p=0.95, top_k=60, num_return_sequences=3)\n",
    "\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Output {i+1}:\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing the Output\n",
    "\n",
    "Improving the coherence and quality of the generated text from a large language model involves fine-tuning various aspects of the model's output settings, and possibly even the model itself. Some strategies are:\n",
    "\n",
    "1. Adjust Sampling Parameters\n",
    "\n",
    "    • **Temperature:** Lowering the temperature value (e.g., between 0.7 to 0.9) can make the text more focused and deterministic, leading to more coherent sentences. However, too low a temperature (e.g., below 0.5) might result in repetitive or overly conservative text.\n",
    "\n",
    "    • **Top-p (Nucleus Sampling):** Top-p sampling (with a value of, e.g., 0.9) considers the smallest set of tokens whose cumulative probability adds up to 0.9. This filters out unlikely options and keeps the text more coherent.\n",
    "\n",
    "    • **Top-k:** Top-k sampling (with a value of, e.g., 50) restricts the number of tokens considered at each step to the top 50, which can help maintain coherence by focusing on the most likely options.\n",
    "    \n",
    "\n",
    "2. Increase the Context Size\n",
    "\n",
    "    • **Longer prompts:** Providing a more detailed prompt or context can guide the model to generate more contextually relevant texts. Instead of starting with a short phrase, you could give a paragraph that sets the scene or introduces key concepts.\n",
    "\n",
    "    • **Sliding window approach:** For generating longer texts, consider feeding the model its previous outputs as part of the prompt in subsequent iterations.\n",
    "\n",
    "\n",
    "3. Fine-Tune the Model on Specific Data\n",
    "\n",
    "    • **Domain-specific fine-tuning:** For specific types of content with high coherence (e.g., technical writing, storytelling), fine-tune the pre-trained model on a dataset that matches desired output style can significantly improve results.\n",
    "\n",
    "    • **Training with Reinforcement Learning:** Techniques like Reinforcement Learning from Human Feedback (RLHF) can be used to adjust the model's behavior based on specific quality metrics.\n",
    "\n",
    "\n",
    "4. Iterative Prompting\n",
    "\n",
    "    • **Iterative prompt refinement:** After generating text, modify the prompt based on the output received and rerun the generation.\n",
    "\n",
    "\n",
    "5. Post-Processing the Generated Text\n",
    "\n",
    "    • **Text editing:** After generating text, you can apply rules or heuristics to clean up or adjust the output. This might include correcting grammar, removing contradictions, or merging sentences for better flow.\n",
    "\n",
    "    • **Chaining outputs:** Generate multiple completions and then manually or programmatically select and combine the best parts. This allows the construction of a more coherent narrative.\n",
    "\n",
    "\n",
    "6. Experiment with Beam Search\n",
    "\n",
    "    • **Beam Search:** Beam search (num_beams=>3) is a technique where the model explores multiple paths before finalizing on the most coherent one. This often leads to better-structured sentences and paragraphs.\n",
    "\n",
    "\n",
    "7. Use Larger or Newer Models\n",
    "\n",
    "    • **Try GPT-3 or GPT-4:** Consider using GPT-3 or GPT-4, which are more advanced and capable of generating contextually appropriate text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps...\n",
    "\n",
    "- Fine-tuning the model on a custom dataset to generate domain-specific text.\n",
    "- Exploring more advanced models like GPT-3 or GPT-4.\n",
    "- Implementing a user interface for interactive text generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
